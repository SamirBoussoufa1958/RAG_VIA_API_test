import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def generate_response(query: str, retrieved_chunks: list, model: str = "gpt-4o", max_tokens: int = 300, temperature: float = 0.7) -> str:
    """
    Generate a response using OpenAI, ensuring context remains within token limits.

    :param query: User's query
    :param retrieved_chunks: List of text chunks retrieved from vector search
    :param model: OpenAI model (default: GPT-4o)
    :param max_tokens: Max output tokens (default: 300)
    :param temperature: Creativity factor (default: 0.7)
    :return: Response generated by the LLM
    """
    try:
        # Limit retrieved context to ~5000 tokens (~20,000 characters)
        context_list = []
        total_length = 0

        for chunk in retrieved_chunks:
            chunk_text = chunk.get("text", "").strip() if isinstance(chunk, dict) else str(chunk).strip()

            if total_length + len(chunk_text) > 20000:  # Ensure token limit
                break

            context_list.append(chunk_text)
            total_length += len(chunk_text)

        context = "\n\n".join(context_list)

        if not context.strip():
            return "No relevant documents found."

        prompt = f"""
        You are an AI assistant that answers queries based on document context.

        Context (truncated to fit token limits):
        {context}

        Query: {query}
        Answer:
        """

        # Debugging

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an expert document assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # Extract response safely
        assistant_response = response.choices[0].message.content.strip() if response.choices else "No response generated."



        return assistant_response

    except Exception as e:

        return f"Error generating response: {e}"
